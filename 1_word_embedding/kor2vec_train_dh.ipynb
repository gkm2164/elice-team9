{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kor2vec train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\easts\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from konlpy.tag import Twitter\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_text, min_count, sampling_rate):\n",
    "    words = list()\n",
    "    for line in desc_list:\n",
    "        sentence = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', line).strip().split()\n",
    "        if sentence:\n",
    "            words.append(sentence)\n",
    "\n",
    "    word_counter = [['UNK', -1]]\n",
    "    word_counter.extend(collections.Counter([word for sentence in words for word in sentence]).most_common())\n",
    "    word_counter = [item for item in word_counter if item[1] >= min_count or item[0] == 'UNK']\n",
    "\n",
    "    word_list = list()\n",
    "    word_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        word_list.append(word) # 학습에 사용된 word를 저장한다. (visualize를 위해)\n",
    "        word_dict[word] = len(word_dict)\n",
    "    word_reverse_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    word_to_pos_li = dict()\n",
    "    pos_list = list()\n",
    "    twitter = Twitter()\n",
    "    for w in word_dict:\n",
    "        w_pos_li = list()\n",
    "        for pos in twitter.pos(w, norm=True):\n",
    "            w_pos_li.append(pos)\n",
    "\n",
    "        word_to_pos_li[word_dict[w]] = w_pos_li\n",
    "        pos_list += w_pos_li\n",
    "\n",
    "    pos_counter = collections.Counter(pos_list).most_common()\n",
    "\n",
    "    pos_dict = dict()\n",
    "    for pos, _ in pos_counter:\n",
    "        pos_dict[pos] = len(pos_dict)\n",
    "\n",
    "    pos_reverse_dict = dict(zip(pos_dict.values(), pos_dict.keys()))\n",
    "\n",
    "    word_to_pos_dict = dict()\n",
    "\n",
    "    for word_id, pos_li in word_to_pos_li.items():\n",
    "        pos_id_li = list()\n",
    "        for pos in pos_li:\n",
    "            pos_id_li.append(pos_dict[pos])\n",
    "        word_to_pos_dict[word_id] = pos_id_li\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for sentence in words:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "            else:\n",
    "                index = word_dict['UNK']\n",
    "                unk_count += 1\n",
    "            s.append(index)\n",
    "        data.append(s)\n",
    "    word_counter[0][1] = max(1, unk_count)\n",
    "\n",
    "    # data = sub_sampling(data, word_counter, word_dict, sampling_rate)\n",
    "\n",
    "    return data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict, word_list\n",
    "\n",
    "def sub_sampling(data, word_counter, word_dict, sampling_rate):\n",
    "    total_words = sum([len(sentence) for sentence in data])\n",
    "    # print(\"total_words: {}\".format(total_words))\n",
    "    prob_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        f = count / total_words # 빈도수가 많을수록 f가 1에 가까워짐.\n",
    "        p = max(0, 1 - math.sqrt(sampling_rate / f)) # sampling_rate가 0.0001이면 f가 클수록 prob이 커진다.\n",
    "        prob_dict[word_dict[word]] = p\n",
    "        # print(\"count : {}, f : {}, p : {}, prob_dict : {}\".format(count, f, p, prob_dict))\n",
    "\n",
    "    new_data = list()\n",
    "    for sentence in data:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            prob = prob_dict[word]\n",
    "            if random.random() > prob: # prob이 작을수록 s에 저장되기 쉬움.\n",
    "                s.append(word)\n",
    "        new_data.append(s)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\easts\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "# crawling한 데이터를 불러온다.\n",
    "with open('../grimm-split.txt', encoding = 'utf-8') as f:\n",
    "    texts = f.readlines()\n",
    "\n",
    "with open('../aesop-split.txt', encoding = 'utf-8') as f:\n",
    "    texts.extend(f.readlines())\n",
    "    \n",
    "desc_list = []\n",
    "for desc in texts:\n",
    "    desc_text = desc.split('\\t')[1]\n",
    "    desc_list.append(desc_text)\n",
    "\n",
    "\n",
    "sampling_rate = 0.0001\n",
    "min_count = 5\n",
    "\n",
    "data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict, word_list \\\n",
    "        = build_dataset(desc_list, min_count, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용된 word list 저장\n",
    "f = open(\"word_list.txt\", 'w', encoding='utf-8')\n",
    "for word in word_list:\n",
    "    input_word = \"{} \".format(word)\n",
    "    f.write(input_word)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences : 522\n",
      "vocabulary size : 6632\n",
      "pos size : 4448\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(word_dict)\n",
    "pos_size = len(pos_dict)\n",
    "num_sentences = len(data)\n",
    "\n",
    "print(\"number of sentences :\", num_sentences)\n",
    "print(\"vocabulary size :\", vocabulary_size)\n",
    "print(\"pos size :\", pos_size)\n",
    "\n",
    "pos_li = []\n",
    "for key in sorted(pos_reverse_dict):\n",
    "    pos_li.append(pos_reverse_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function to generate a training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "batch_size = 150\n",
    "\n",
    "# kor2vec 의 input index list와 output index list를 만든다.\n",
    "# 윈도우 사이즈에 따라 input output pair가 늘어난다.(input이 중복)\n",
    "def generate_input_output_list(data, window_size):\n",
    "    input_li = list()\n",
    "    output_li = list()\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    if sentence[i]!=word_dict['UNK'] and sentence[j]!=word_dict['UNK']:\n",
    "                        input_li.append(sentence[i])\n",
    "                        output_li.append(sentence[j])\n",
    "    return input_li, output_li\n",
    "\n",
    "input_li, output_li = generate_input_output_list(data, window_size)\n",
    "input_li_size = len(input_li)\n",
    "\n",
    "# 확인\n",
    "# for i in range(input_li_size):\n",
    "#     print(\"-{}-\".format(i)) \n",
    "#     in_index = word_to_pos_dict[input_li[i]]\n",
    "#     out_index = word_to_pos_dict[output_li[i]]\n",
    "#     print(in_index)\n",
    "#     for ind in in_index:\n",
    "#         print(pos_reverse_dict[ind])\n",
    "#     print(out_index)\n",
    "#     for o in out_index:\n",
    "#         print(pos_reverse_dict[o])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "1311002\n",
      "(150,)\n",
      "[ 488  488  488  652  652  652  652 1419 1419 1419 1419 1419 1028 1028\n",
      " 1028 1028 1028 1028  426  426  426  426  426  426 4167 4167 4167 4167\n",
      " 4167 4167 4167 4167   84   84   84   84   84   84   84   84  353  353\n",
      "  353  353  353  353  353  353   28   28   28   28   28   28   28   28\n",
      "    3    3    3    3    3    3    3    3 1879 1879 1879 1879 1879 1879\n",
      " 1879 1879 1879  239  239  239  239  239  239  239  239  851  851  851\n",
      "  851  851  851  851  851 3247 3247 3247 3247 3247 3247 3247 3247  107\n",
      "  107  107  107  107  107  107  107  326  326  326  326  326  326  326\n",
      " 5854 5854 5854 5854 5854 5854  881  881  881  881  881  881   73   73\n",
      "   73   73   73   73 1192 1192 1192 1192 1192 1192  837  837  837  837\n",
      "  837  837  837 1383 1383 1383 1383 1383 1383 1383]\n",
      "(150, 1)\n",
      "[[ 652]\n",
      " [1419]\n",
      " [1028]\n",
      " [ 488]\n",
      " [1419]\n",
      " [1028]\n",
      " [ 426]\n",
      " [ 488]\n",
      " [ 652]\n",
      " [1028]\n",
      " [ 426]\n",
      " [4167]\n",
      " [ 488]\n",
      " [ 652]\n",
      " [1419]\n",
      " [ 426]\n",
      " [4167]\n",
      " [  84]\n",
      " [ 652]\n",
      " [1419]\n",
      " [1028]\n",
      " [4167]\n",
      " [  84]\n",
      " [ 353]\n",
      " [1419]\n",
      " [1028]\n",
      " [ 426]\n",
      " [  84]\n",
      " [ 353]\n",
      " [  28]\n",
      " [   3]\n",
      " [1879]\n",
      " [1028]\n",
      " [ 426]\n",
      " [4167]\n",
      " [ 353]\n",
      " [  28]\n",
      " [   3]\n",
      " [1879]\n",
      " [ 239]\n",
      " [ 426]\n",
      " [4167]\n",
      " [  84]\n",
      " [  28]\n",
      " [   3]\n",
      " [1879]\n",
      " [ 239]\n",
      " [ 851]\n",
      " [4167]\n",
      " [  84]\n",
      " [ 353]\n",
      " [   3]\n",
      " [1879]\n",
      " [ 239]\n",
      " [ 851]\n",
      " [3247]\n",
      " [4167]\n",
      " [  84]\n",
      " [ 353]\n",
      " [  28]\n",
      " [1879]\n",
      " [ 239]\n",
      " [ 851]\n",
      " [3247]\n",
      " [4167]\n",
      " [  84]\n",
      " [ 353]\n",
      " [  28]\n",
      " [   3]\n",
      " [ 239]\n",
      " [ 851]\n",
      " [3247]\n",
      " [ 107]\n",
      " [  84]\n",
      " [ 353]\n",
      " [  28]\n",
      " [   3]\n",
      " [1879]\n",
      " [ 851]\n",
      " [3247]\n",
      " [ 107]\n",
      " [ 353]\n",
      " [  28]\n",
      " [   3]\n",
      " [1879]\n",
      " [ 239]\n",
      " [3247]\n",
      " [ 107]\n",
      " [ 326]\n",
      " [  28]\n",
      " [   3]\n",
      " [1879]\n",
      " [ 239]\n",
      " [ 851]\n",
      " [ 107]\n",
      " [ 326]\n",
      " [5854]\n",
      " [1879]\n",
      " [ 239]\n",
      " [ 851]\n",
      " [3247]\n",
      " [ 326]\n",
      " [5854]\n",
      " [ 881]\n",
      " [  73]\n",
      " [ 851]\n",
      " [3247]\n",
      " [ 107]\n",
      " [5854]\n",
      " [ 881]\n",
      " [  73]\n",
      " [1192]\n",
      " [3247]\n",
      " [ 107]\n",
      " [ 326]\n",
      " [ 881]\n",
      " [  73]\n",
      " [1192]\n",
      " [ 107]\n",
      " [ 326]\n",
      " [5854]\n",
      " [  73]\n",
      " [1192]\n",
      " [ 837]\n",
      " [ 107]\n",
      " [ 326]\n",
      " [5854]\n",
      " [ 881]\n",
      " [1192]\n",
      " [ 837]\n",
      " [ 326]\n",
      " [5854]\n",
      " [ 881]\n",
      " [  73]\n",
      " [ 837]\n",
      " [1383]\n",
      " [ 881]\n",
      " [  73]\n",
      " [1192]\n",
      " [1383]\n",
      " [4299]\n",
      " [ 322]\n",
      " [3368]\n",
      " [1192]\n",
      " [ 837]\n",
      " [4299]\n",
      " [ 322]\n",
      " [3368]\n",
      " [ 704]\n",
      " [ 258]]\n",
      "[[808], [808], [808], [2801, 3], [2801, 3], [2801, 3], [2801, 3], [2756], [2756], [2756], [2756], [2756], [162, 1], [162, 1], [162, 1], [162, 1], [162, 1], [162, 1], [2692], [2692], [2692], [2692], [2692], [2692], [80, 5, 1], [80, 5, 1], [80, 5, 1], [80, 5, 1], [80, 5, 1], [80, 5, 1], [80, 5, 1], [80, 5, 1], [216], [216], [216], [216], [216], [216], [216], [216], [1936], [1936], [1936], [1936], [1936], [1936], [1936], [1936], [49, 1], [49, 1], [49, 1], [49, 1], [49, 1], [49, 1], [49, 1], [49, 1], [2649], [2649], [2649], [2649], [2649], [2649], [2649], [2649], [220], [220], [220], [220], [220], [220], [220], [220], [220], [210, 18], [210, 18], [210, 18], [210, 18], [210, 18], [210, 18], [210, 18], [210, 18], [2867], [2867], [2867], [2867], [2867], [2867], [2867], [2867], [699, 71], [699, 71], [699, 71], [699, 71], [699, 71], [699, 71], [699, 71], [699, 71], [2031], [2031], [2031], [2031], [2031], [2031], [2031], [2031], [1207], [1207], [1207], [1207], [1207], [1207], [1207], [2065], [2065], [2065], [2065], [2065], [2065], [4298], [4298], [4298], [4298], [4298], [4298], [29, 6], [29, 6], [29, 6], [29, 6], [29, 6], [29, 6], [179, 3], [179, 3], [179, 3], [179, 3], [179, 3], [179, 3], [46, 804], [46, 804], [46, 804], [46, 804], [46, 804], [46, 804], [46, 804], [2060], [2060], [2060], [2060], [2060], [2060], [2060]]\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)\n",
    "print(input_li_size)\n",
    "def generate_batch(iter, batch_size, input_li, output_li):\n",
    "    index = (iter % (input_li_size//batch_size)) * batch_size\n",
    "    batch_input = input_li[index:index+batch_size]\n",
    "    batch_output_li = output_li[index:index+batch_size]\n",
    "    batch_output = [[i] for i in batch_output_li]\n",
    "\n",
    "    return np.array(batch_input), np.array(batch_output)\n",
    "\n",
    "batch_inputs, batch_labels = generate_batch(0, batch_size, input_li, output_li)\n",
    "print(np.shape(batch_inputs))\n",
    "print(batch_inputs)\n",
    "print(np.shape(batch_labels))\n",
    "print(batch_labels)\n",
    "word_list = []\n",
    "for word in batch_inputs:\n",
    "    word_list.append(word_to_pos_dict[word])\n",
    "print(word_list)\n",
    "#     for pos in word_to_pos_dict[word]:\n",
    "#         print(pos)\n",
    "#         print(pos_reverse_dict[pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 150\n",
    "num_sampled = 50\n",
    "learning_rate = 1.0\n",
    "\n",
    "valid_size = 20     # Random set of words to evaluate similarity on.\n",
    "valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False) # 200까지 숫자 중에서 랜덤하게 20개 뽑음\n",
    "\n",
    "# tensorflow 신경망 모델 그래프 생성\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    words_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(batch_size)] # batch_size만큼의 word를 형태소로\n",
    "    vocabulary_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(vocabulary_size)] # word_dict만큼의 word를 형태소로.. 인거 같은데 안씀\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # \"/device:GPU:0\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        # embedding vector -> 우리가 원하는 최종 출력\n",
    "        pos_embeddings = tf.Variable(tf.random_uniform([pos_size, embedding_size], -1.0, 1.0), name='pos_embeddings')\n",
    "\n",
    "        word_vec_list = []\n",
    "        for i in range(batch_size):\n",
    "            word_vec = tf.reduce_sum(tf.nn.embedding_lookup(pos_embeddings, words_matrix[i]), 0)\n",
    "            word_vec_list.append(word_vec)\n",
    "        word_embeddings = tf.stack(word_vec_list) # word의 각 형태소를 embedding한 vector\n",
    "    \n",
    "        # Noise-Contrastive Estimation\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)), name='nce_weights'\n",
    "        )\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]), name='nce_biases')\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=word_embeddings,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Compute the cosine similarity between minibatch exaples and all embeddings.\n",
    "    # 임의의 word로 유사도 검증\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(pos_embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = pos_embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations for each epoch : 8740\n",
      "Initialized - Tensorflow\n",
      "Average loss at step  0 :  182.2454833984375\n",
      "Nearest to ('위', 'Noun'): ('엄청', 'Adverb'), ('부드럽게', 'Adjective'), ('땅', 'Noun'), ('매달린', 'Verb'), ('쉬', 'Noun'), ('생쥐', 'Noun'), ('에겐', 'Josa'), ('싸우고', 'Verb'),\n",
      "Nearest to ('산토끼', 'Noun'): ('가질', 'Verb'), ('눈앞', 'Noun'), ('잠들어', 'Verb'), ('들이', 'Verb'), ('밭', 'Noun'), ('푼', 'Noun'), ('다가올', 'Verb'), ('라면', 'Noun'),\n",
      "Nearest to ('나', 'Noun'): ('그야말로', 'Adverb'), ('이래도', 'Josa'), ('처녀', 'Noun'), ('뚝', 'Noun'), ('온', 'Noun'), ('만나', 'Verb'), ('2시간', 'Number'), ('죽는', 'Verb'),\n",
      "Nearest to ('음식', 'Noun'): ('오셔서', 'Verb'), ('전', 'Noun'), ('몰랐어요', 'Verb'), ('잤어요', 'Verb'), ('마주치게', 'Verb'), ('가겠다', 'Verb'), ('옳거니', 'Adjective'), ('전해', 'Noun'),\n",
      "Nearest to ('번', 'Noun'): ('차마', 'Noun'), ('도둑', 'Noun'), ('터지게', 'Verb'), ('마왕', 'Noun'), ('없으니', 'Adjective'), ('찔리고', 'Verb'), ('걷고', 'Verb'), ('번째', 'Suffix'),\n",
      "Nearest to ('식탁', 'Noun'): ('를', 'Foreign'), ('용의', 'Noun'), ('놀라며', 'Verb'), ('딱', 'Adverb'), ('요하네스', 'Noun'), ('안대', 'Noun'), ('빌고', 'Verb'), ('저', 'Determiner'),\n",
      "Nearest to ('아빠', 'Noun'): ('퍽', 'Noun'), ('자넨', 'Noun'), ('하', 'Suffix'), ('되나요', 'Verb'), ('없기', 'Adjective'), ('가능한', 'Adjective'), ('히', 'Adverb'), ('멍청한', 'Adjective'),\n",
      "Nearest to ('해', 'Verb'): ('주곤', 'Verb'), ('에구', 'Exclamation'), ('똥찬', 'Noun'), ('대', 'Verb'), ('교활한', 'Adjective'), ('꽉', 'Noun'), ('죽자', 'Verb'), ('올리는', 'Verb'),\n",
      "Nearest to ('과', 'Josa'): ('덩달아', 'Noun'), ('죽음', 'Noun'), ('그러다', 'Adjective'), ('꼬', 'Noun'), ('일찍이', 'Noun'), ('얌전히', 'Adjective'), ('주문', 'Noun'), ('과연', 'Noun'),\n",
      "Nearest to ('속', 'Noun'): ('날아가는', 'Verb'), ('달렸어요', 'Verb'), ('돌아온', 'Verb'), ('싸', 'Verb'), ('표정', 'Noun'), ('목숨', 'Noun'), ('니', 'Josa'), ('난장', 'Noun'),\n",
      "Nearest to ('새', 'Noun'): ('암탉', 'Noun'), ('싶습니다', 'Verb'), ('가요', 'Noun'), ('피부', 'Noun'), ('때리며', 'Verb'), ('데려다주었어요', 'Verb'), ('모른', 'Verb'), ('하얀', 'Adjective'),\n",
      "Nearest to ('우리', 'Noun'): ('바람', 'Noun'), ('돌', 'Noun'), ('며', 'Josa'), ('망할', 'Adjective'), ('호주머니', 'Noun'), ('싶음', 'Verb'), ('척', 'Noun'), ('당할', 'Adjective'),\n",
      "Nearest to ('지', 'Josa'): ('있는가', 'Adjective'), ('왔고', 'Verb'), ('올려놓고', 'Verb'), ('느끼곤', 'Verb'), ('추운', 'Verb'), ('꽃', 'Noun'), ('아세요', 'Verb'), ('있기에', 'Adjective'),\n",
      "Nearest to ('그', 'Noun'): ('즐겁고', 'Adjective'), ('소용', 'Noun'), ('샐러드', 'Noun'), ('번째', 'Suffix'), ('건넸어요', 'Verb'), ('갔을', 'Verb'), ('미모', 'Noun'), ('찔리고', 'Verb'),\n",
      "Nearest to ('길', 'Noun'): ('마시면', 'Verb'), ('내는', 'Verb'), ('엄청난', 'Adjective'), ('쌓여', 'Verb'), ('따라', 'Verb'), ('변한', 'Adjective'), ('아니지', 'Adjective'), ('동정녀', 'Noun'),\n",
      "Nearest to ('집', 'Noun'): ('불어', 'Noun'), ('대한', 'Noun'), ('갉아먹고', 'Verb'), ('항해', 'Noun'), ('나갈', 'Verb'), ('반드시', 'Noun'), ('종', 'Noun'), ('중', 'Noun'),\n",
      "Nearest to ('명', 'Noun'): ('아니오', 'Adjective'), ('있겠어요', 'Adjective'), ('그리며', 'Verb'), ('콩', 'Noun'), ('내밀며', 'Verb'), ('촛불', 'Noun'), ('보내기로', 'Verb'), ('내는', 'Verb'),\n",
      "Nearest to ('하나', 'Noun'): ('돌아와야', 'Verb'), ('자란', 'Verb'), ('어머나', 'Noun'), ('앙앙', 'Adverb'), ('밤', 'Noun'), ('땋', 'Noun'), ('대로', 'Noun'), ('주일', 'Noun'),\n",
      "Nearest to ('선', 'Noun'): ('달리는', 'Verb'), ('아주', 'Noun'), ('바늘', 'Noun'), ('머릿속', 'Noun'), ('자러', 'Verb'), ('곰', 'Noun'), ('파고', 'Noun'), ('뛸', 'Verb'),\n",
      "Nearest to ('방', 'Noun'): ('봤나', 'Verb'), ('자른', 'Verb'), ('태양', 'Noun'), ('살았어요', 'Verb'), ('턱', 'Noun'), ('항아리', 'Noun'), ('토막', 'Noun'), ('달려들었어요', 'Verb'),\n",
      "Average loss at step  8740 :  51.277936594843865\n",
      "Average loss at step  17480 :  22.599888768136502\n",
      "Nearest to ('위', 'Noun'): ('엄청', 'Adverb'), ('앉아', 'Verb'), ('한데', 'Eomi'), ('숨었고', 'Verb'), ('쉬', 'Noun'), ('남자', 'Noun'), ('생쥐', 'Noun'), ('달려들어', 'Verb'),\n",
      "Nearest to ('산토끼', 'Noun'): ('그러자', 'Conjunction'), ('늑대', 'Noun'), ('잠들어', 'Verb'), ('기껏', 'Noun'), ('웬', 'Noun'), ('사과나무', 'Noun'), ('한', 'Verb'), ('이', 'Noun'),\n",
      "Nearest to ('나', 'Noun'): ('이', 'Noun'), ('덜커덩', 'Noun'), ('라며', 'Noun'), ('내', 'Noun'), ('퍽', 'Noun'), ('아크', 'Noun'), ('말', 'Noun'), ('그', 'Noun'),\n",
      "Nearest to ('음식', 'Noun'): ('마주치게', 'Verb'), ('잤어요', 'Verb'), ('아프던', 'Adjective'), ('옳거니', 'Adjective'), ('ㅜ', 'KoreanParticle'), ('가', 'Verb'), ('넣고', 'Verb'), ('전해', 'Noun'),\n",
      "Nearest to ('번', 'Noun'): ('나고', 'Verb'), ('가졌어요', 'Verb'), ('죽은', 'Verb'), ('번째', 'Suffix'), ('없으니', 'Adjective'), ('찔리고', 'Verb'), ('그', 'Noun'), ('때', 'Noun'),\n",
      "Nearest to ('식탁', 'Noun'): ('그', 'Noun'), ('기', 'Modifier'), ('용의', 'Noun'), ('딱', 'Adverb'), ('먹지도', 'Verb'), ('있잖아요', 'Adjective'), ('왕자', 'Noun'), ('놀라며', 'Verb'),\n",
      "Nearest to ('아빠', 'Noun'): ('내', 'Noun'), ('왔고', 'Verb'), ('이', 'Noun'), ('되나요', 'Verb'), ('네', 'Noun'), ('말', 'Noun'), ('퍽', 'Noun'), ('하지만', 'Conjunction'),\n",
      "Nearest to ('해', 'Verb'): ('주곤', 'Verb'), ('꽉', 'Noun'), ('에구', 'Exclamation'), ('대', 'Verb'), ('된다는', 'Verb'), ('교활한', 'Adjective'), ('때리며', 'Verb'), ('다다르게', 'Verb'),\n",
      "Nearest to ('과', 'Josa'): ('그러다', 'Adjective'), ('동안', 'Noun'), ('가두고', 'Verb'), ('죽음', 'Noun'), ('음료수', 'Noun'), ('짚', 'Noun'), ('있었어요', 'Adjective'), ('온전히', 'Verb'),\n",
      "Nearest to ('속', 'Noun'): ('그', 'Noun'), ('가졌어요', 'Verb'), ('덜커덩', 'Noun'), ('날아가는', 'Verb'), ('숨었고', 'Verb'), ('이', 'Noun'), ('대체', 'Noun'), ('띄지', 'Verb'),\n",
      "Nearest to ('새', 'Noun'): ('암탉', 'Noun'), ('마리', 'Noun'), ('왔고', 'Verb'), ('있는', 'Adjective'), ('하나', 'Noun'), ('한', 'Verb'), ('그', 'Noun'), ('두', 'Noun'),\n",
      "Nearest to ('우리', 'Noun'): ('난', 'Noun'), ('다', 'Adverb'), ('라며', 'Noun'), ('저', 'Noun'), ('갈', 'Verb'), ('숨었고', 'Verb'), ('가지', 'Noun'), ('그리고', 'Conjunction'),\n",
      "Nearest to ('지', 'Josa'): ('윙윙', 'Noun'), ('느끼곤', 'Verb'), ('슬쩍', 'Adverb'), ('과연', 'Noun'), ('있기에', 'Adjective'), ('든', 'Verb'), ('왔고', 'Verb'), ('그랬다간', 'Adjective'),\n",
      "Nearest to ('그', 'Noun'): ('그녀', 'Noun'), ('하지만', 'Conjunction'), ('덜커덩', 'Noun'), ('다시', 'Noun'), ('잡으세요', 'Verb'), ('그래서', 'Adverb'), ('이', 'Noun'), ('왔고', 'Verb'),\n",
      "Nearest to ('길', 'Noun'): ('그', 'Noun'), ('그녀', 'Noun'), ('하지만', 'Conjunction'), ('따라', 'Verb'), ('그렇게', 'Adverb'), ('덜커덩', 'Noun'), ('총', 'Noun'), ('집의', 'Noun'),\n",
      "Nearest to ('집', 'Noun'): ('왔고', 'Verb'), ('잡으세요', 'Verb'), ('왕', 'Noun'), ('숨었고', 'Verb'), ('그', 'Noun'), ('덜커덩', 'Noun'), ('하지만', 'Conjunction'), ('그녀', 'Noun'),\n",
      "Nearest to ('명', 'Noun'): ('한', 'Verb'), ('포도주', 'Noun'), ('마리', 'Noun'), ('아니오', 'Adjective'), ('더', 'Noun'), ('보내기로', 'Verb'), ('그리며', 'Verb'), ('있겠어요', 'Adjective'),\n",
      "Nearest to ('하나', 'Noun'): ('한', 'Verb'), ('있는', 'Adjective'), ('덜커덩', 'Noun'), ('숨었고', 'Verb'), ('잡으세요', 'Verb'), ('그래서', 'Adverb'), ('그', 'Noun'), ('앙앙', 'Adverb'),\n",
      "Nearest to ('선', 'Noun'): ('달리는', 'Verb'), ('아주', 'Noun'), ('거', 'Noun'), ('파고', 'Noun'), ('자러', 'Verb'), ('임무', 'Noun'), ('걸', 'Noun'), ('게', 'Noun'),\n",
      "Nearest to ('방', 'Noun'): ('또', 'Noun'), ('봤나', 'Verb'), ('그녀', 'Noun'), ('아셨죠', 'Verb'), ('시간', 'Noun'), ('자른', 'Verb'), ('거기', 'Noun'), ('토막', 'Noun'),\n",
      "Average loss at step  26220 :  21.847575735986233\n",
      "Average loss at step  34960 :  21.4056757414341\n",
      "Average loss at step  43700 :  21.052079868495465\n",
      "Nearest to ('위', 'Noun'): ('엄청', 'Adverb'), ('보니', 'Verb'), ('아래', 'Noun'), ('숨었고', 'Verb'), ('앉아', 'Verb'), ('위로', 'Noun'), ('달려들어', 'Verb'), ('쉬', 'Noun'),\n",
      "Nearest to ('산토끼', 'Noun'): ('늑대', 'Noun'), ('기껏', 'Noun'), ('잠들어', 'Verb'), ('웬', 'Noun'), ('그러자', 'Conjunction'), ('그때', 'Noun'), ('자르고', 'Verb'), ('푼', 'Noun'),\n",
      "Nearest to ('나', 'Noun'): ('내', 'Noun'), ('이', 'Noun'), ('다', 'Adverb'), ('퍽', 'Noun'), ('라며', 'Noun'), ('그', 'Noun'), ('피해', 'Noun'), ('덜커덩', 'Noun'),\n",
      "Nearest to ('음식', 'Noun'): ('잤어요', 'Verb'), ('지쳐', 'Verb'), ('마주치게', 'Verb'), ('피워', 'Verb'), ('아뿔싸', 'Noun'), ('넣고', 'Verb'), ('먹고', 'Verb'), ('사형', 'Noun'),\n",
      "Nearest to ('번', 'Noun'): ('나고', 'Verb'), ('죽은', 'Verb'), ('다시', 'Noun'), ('가졌어요', 'Verb'), ('을', 'Josa'), ('하지만', 'Conjunction'), ('찔리고', 'Verb'), ('제', 'Noun'),\n",
      "Nearest to ('식탁', 'Noun'): ('있잖아요', 'Adjective'), ('요하네스', 'Noun'), ('먹지도', 'Verb'), ('기', 'Modifier'), ('거대한', 'Adjective'), ('놀라며', 'Verb'), ('딱', 'Adverb'), ('용의', 'Noun'),\n",
      "Nearest to ('아빠', 'Noun'): ('그', 'Noun'), ('네', 'Noun'), ('되나요', 'Verb'), ('엄마', 'Noun'), ('이', 'Noun'), ('왔고', 'Verb'), ('내', 'Noun'), ('그래', 'Adjective'),\n",
      "Nearest to ('해', 'Verb'): ('주곤', 'Verb'), ('그러자', 'Conjunction'), ('된다는', 'Verb'), ('꽉', 'Noun'), ('퍼', 'PreEomi'), ('에구', 'Exclamation'), ('막히게', 'Verb'), ('대', 'Verb'),\n",
      "Nearest to ('과', 'Josa'): ('그러다', 'Adjective'), ('가두고', 'Verb'), ('추었답니다', 'Verb'), ('불성실한', 'Adjective'), ('함께', 'Adverb'), ('않니', 'Verb'), ('괜히', 'Adverb'), ('옛적', 'Noun'),\n",
      "Nearest to ('속', 'Noun'): ('가졌어요', 'Verb'), ('들어갔어요', 'Verb'), ('진기한', 'Adjective'), ('대체', 'Noun'), ('영리한', 'Adjective'), ('숨었고', 'Verb'), ('없겠다', 'Adjective'), ('있던', 'Adjective'),\n",
      "Nearest to ('새', 'Noun'): ('암탉', 'Noun'), ('마리', 'Noun'), ('왔고', 'Verb'), ('그때', 'Noun'), ('하얀', 'Adjective'), ('모른', 'Verb'), ('그', 'Noun'), ('이', 'Noun'),\n",
      "Nearest to ('우리', 'Noun'): ('난', 'Noun'), ('내', 'Noun'), ('다', 'Adverb'), ('라며', 'Noun'), ('갈', 'Verb'), ('가지', 'Noun'), ('했어요', 'Verb'), ('집', 'Noun'),\n",
      "Nearest to ('지', 'Josa'): ('간청', 'Noun'), ('윙윙', 'Noun'), ('올라갔지요', 'Verb'), ('느끼곤', 'Verb'), ('그랬다간', 'Adjective'), ('맞지', 'Verb'), ('추운', 'Verb'), ('슬쩍', 'Adverb'),\n",
      "Nearest to ('그', 'Noun'): ('그녀', 'Noun'), ('이', 'Noun'), ('덜커덩', 'Noun'), ('을', 'Josa'), ('잡으세요', 'Verb'), ('왔고', 'Verb'), ('숨었고', 'Verb'), ('자신', 'Noun'),\n",
      "Nearest to ('길', 'Noun'): ('따라', 'Verb'), ('집의', 'Noun'), ('그렇게', 'Adverb'), ('변한', 'Adjective'), ('총', 'Noun'), ('그러다', 'Adjective'), ('숲', 'Noun'), ('그', 'Noun'),\n",
      "Nearest to ('집', 'Noun'): ('함께', 'Adverb'), ('어둠', 'Noun'), ('한동안', 'Adverb'), ('가', 'Verb'), ('우리', 'Noun'), ('둘', 'Noun'), ('잡으세요', 'Verb'), ('여길', 'Verb'),\n",
      "Nearest to ('명', 'Noun'): ('한', 'Verb'), ('마리', 'Noun'), ('포도주', 'Noun'), ('가난한', 'Adjective'), ('그', 'Noun'), ('있겠어요', 'Adjective'), ('있었어요', 'Adjective'), ('그리며', 'Verb'),\n",
      "Nearest to ('하나', 'Noun'): ('한', 'Verb'), ('작은', 'Adjective'), ('숨었고', 'Verb'), ('개', 'Noun'), ('덜커덩', 'Noun'), ('돌아와야', 'Verb'), ('앙앙', 'Adverb'), ('있는', 'Adjective'),\n",
      "Nearest to ('선', 'Noun'): ('달리는', 'Verb'), ('아주', 'Noun'), ('파고', 'Noun'), ('작은', 'Adjective'), ('나무', 'Noun'), ('자러', 'Verb'), ('거', 'Noun'), ('바늘', 'Noun'),\n",
      "Nearest to ('방', 'Noun'): ('또', 'Noun'), ('봤나', 'Verb'), ('거기', 'Noun'), ('기어', 'Noun'), ('에게', 'Josa'), ('성은', 'Noun'), ('살아있는', 'Verb'), ('아셨죠', 'Verb'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52440 :  20.81737487030029\n",
      "Average loss at step  61180 :  20.599755706518888\n",
      "Nearest to ('위', 'Noun'): ('엄청', 'Adverb'), ('달려들어', 'Verb'), ('앉아', 'Verb'), ('위로', 'Noun'), ('아래', 'Noun'), ('보니', 'Verb'), ('숨었고', 'Verb'), ('선물', 'Noun'),\n",
      "Nearest to ('산토끼', 'Noun'): ('늑대', 'Noun'), ('그러자', 'Conjunction'), ('기껏', 'Noun'), ('그때', 'Noun'), ('여우', 'Noun'), ('내며', 'Verb'), ('잠들어', 'Verb'), ('푼', 'Noun'),\n",
      "Nearest to ('나', 'Noun'): ('퍽', 'Noun'), ('그', 'Noun'), ('내', 'Noun'), ('이', 'Noun'), ('다', 'Adverb'), ('소리', 'Noun'), ('이래도', 'Josa'), ('라며', 'Noun'),\n",
      "Nearest to ('음식', 'Noun'): ('먹고', 'Verb'), ('잤어요', 'Verb'), ('피워', 'Verb'), ('지쳐', 'Verb'), ('이', 'Noun'), ('아뿔싸', 'Noun'), ('펼쳐', 'Verb'), ('넣고', 'Verb'),\n",
      "Nearest to ('번', 'Noun'): ('나고', 'Verb'), ('다시', 'Noun'), ('하지만', 'Conjunction'), ('죽은', 'Verb'), ('힘든', 'Adjective'), ('지으며', 'Verb'), ('대지', 'Noun'), ('더', 'Noun'),\n",
      "Nearest to ('식탁', 'Noun'): ('있잖아요', 'Adjective'), ('상', 'Noun'), ('일', 'Noun'), ('거대한', 'Adjective'), ('그', 'Noun'), ('요하네스', 'Noun'), ('먹지도', 'Verb'), ('기', 'Modifier'),\n",
      "Nearest to ('아빠', 'Noun'): ('엄마', 'Noun'), ('되나요', 'Verb'), ('네', 'Noun'), ('따르는', 'Verb'), ('내', 'Noun'), ('그래', 'Adjective'), ('집', 'Noun'), ('아들', 'Noun'),\n",
      "Nearest to ('해', 'Verb'): ('주곤', 'Verb'), ('퍼', 'PreEomi'), ('된다는', 'Verb'), ('꽉', 'Noun'), ('막히게', 'Verb'), ('않소', 'Verb'), ('않습니다', 'Verb'), ('대', 'Verb'),\n",
      "Nearest to ('과', 'Josa'): ('함께', 'Adverb'), ('와', 'Josa'), ('가두고', 'Verb'), ('똑같은', 'Adjective'), ('그러다', 'Adjective'), ('옛적', 'Noun'), ('않니', 'Verb'), ('추었답니다', 'Verb'),\n",
      "Nearest to ('속', 'Noun'): ('가졌어요', 'Verb'), ('영리한', 'Adjective'), ('없겠다', 'Adjective'), ('진기한', 'Adjective'), ('가련한', 'Adjective'), ('그래도', 'Adverb'), ('들어갔어요', 'Verb'), ('그', 'Noun'),\n",
      "Nearest to ('새', 'Noun'): ('하얀', 'Adjective'), ('마리', 'Noun'), ('암탉', 'Noun'), ('첨벙', 'Noun'), ('묶음', 'Noun'), ('주던', 'Verb'), ('아름다운', 'Adjective'), ('모른', 'Verb'),\n",
      "Nearest to ('우리', 'Noun'): ('난', 'Noun'), ('라며', 'Noun'), ('다', 'Adverb'), ('어서', 'Noun'), ('갈', 'Verb'), ('내', 'Noun'), ('집', 'Noun'), ('가는', 'Verb'),\n",
      "Nearest to ('지', 'Josa'): ('간청', 'Noun'), ('윙윙', 'Noun'), ('왔고', 'Verb'), ('알아보곤', 'Verb'), ('올라갔지요', 'Verb'), ('느끼곤', 'Verb'), ('아세요', 'Verb'), ('추운', 'Verb'),\n",
      "Nearest to ('그', 'Noun'): ('그녀', 'Noun'), ('하지만', 'Conjunction'), ('이', 'Noun'), ('공주', 'Noun'), ('다시', 'Noun'), ('그래서', 'Adverb'), ('큰', 'Verb'), ('이', 'Josa'),\n",
      "Nearest to ('길', 'Noun'): ('따라', 'Verb'), ('그렇게', 'Adverb'), ('가는', 'Verb'), ('집의', 'Noun'), ('숲', 'Noun'), ('그', 'Noun'), ('그러다', 'Adjective'), ('변한', 'Adjective'),\n",
      "Nearest to ('집', 'Noun'): ('빌', 'Verb'), ('아빠', 'Noun'), ('함께', 'Adverb'), ('어둠', 'Noun'), ('우리', 'Noun'), ('오늘', 'Noun'), ('한동안', 'Adverb'), ('여길', 'Verb'),\n",
      "Nearest to ('명', 'Noun'): ('한', 'Verb'), ('마리', 'Noun'), ('가난한', 'Adjective'), ('중', 'Noun'), ('더', 'Noun'), ('살', 'Noun'), ('포도주', 'Noun'), ('있겠어요', 'Adjective'),\n",
      "Nearest to ('하나', 'Noun'): ('한', 'Verb'), ('개', 'Noun'), ('작은', 'Adjective'), ('나무', 'Noun'), ('있는', 'Adjective'), ('놓여', 'Verb'), ('돌아와야', 'Verb'), ('어떤', 'Adjective'),\n",
      "Nearest to ('선', 'Noun'): ('달리는', 'Verb'), ('파고', 'Noun'), ('재봉사', 'Noun'), ('아주', 'Noun'), ('도망', 'Noun'), ('나무', 'Noun'), ('바늘', 'Noun'), ('자러', 'Verb'),\n",
      "Nearest to ('방', 'Noun'): ('또', 'Noun'), ('보더', 'Noun'), ('기어', 'Noun'), ('있었죠', 'Adjective'), ('거기', 'Noun'), ('지르고', 'Verb'), ('봤나', 'Verb'), ('자르고', 'Verb'),\n",
      "Average loss at step  69920 :  20.45169868260622\n",
      "Average loss at step  78660 :  20.257169282734395\n",
      "Average loss at step  87400 :  20.12713357448578\n",
      "Nearest to ('위', 'Noun'): ('달려들어', 'Verb'), ('엄청', 'Adverb'), ('위로', 'Noun'), ('보니', 'Verb'), ('그대', 'Noun'), ('앉아', 'Verb'), ('속', 'Noun'), ('숨었고', 'Verb'),\n",
      "Nearest to ('산토끼', 'Noun'): ('늑대', 'Noun'), ('기껏', 'Noun'), ('그러자', 'Conjunction'), ('달려들었어요', 'Verb'), ('자르고', 'Verb'), ('그때', 'Noun'), ('푼', 'Noun'), ('내며', 'Verb'),\n",
      "Nearest to ('나', 'Noun'): ('퍽', 'Noun'), ('내', 'Noun'), ('이래도', 'Josa'), ('이', 'Noun'), ('다', 'Adverb'), ('더', 'Noun'), ('소리', 'Noun'), ('라며', 'Noun'),\n",
      "Nearest to ('음식', 'Noun'): ('먹고', 'Verb'), ('피워', 'Verb'), ('아뿔싸', 'Noun'), ('펼쳐', 'Verb'), ('지쳐', 'Verb'), ('잤어요', 'Verb'), ('넣고', 'Verb'), ('빠져나올', 'Verb'),\n",
      "Nearest to ('번', 'Noun'): ('하지만', 'Conjunction'), ('나고', 'Verb'), ('힘든', 'Adjective'), ('입도', 'Noun'), ('치의', 'Noun'), ('됐네', 'Verb'), ('다시', 'Noun'), ('더', 'Noun'),\n",
      "Nearest to ('식탁', 'Noun'): ('상', 'Noun'), ('있잖아요', 'Adjective'), ('일', 'Noun'), ('거대한', 'Adjective'), ('요하네스', 'Noun'), ('지키고', 'Verb'), ('년', 'Noun'), ('먹지도', 'Verb'),\n",
      "Nearest to ('아빠', 'Noun'): ('엄마', 'Noun'), ('아들', 'Noun'), ('그래', 'Adjective'), ('되나요', 'Verb'), ('따르는', 'Verb'), ('한껏', 'Adverb'), ('없기', 'Adjective'), ('에게', 'Josa'),\n",
      "Nearest to ('해', 'Verb'): ('퍼', 'PreEomi'), ('주곤', 'Verb'), ('된다는', 'Verb'), ('않소', 'Verb'), ('막히게', 'Verb'), ('꽉', 'Noun'), ('바라보며', 'Verb'), ('자루', 'Noun'),\n",
      "Nearest to ('과', 'Josa'): ('와', 'Josa'), ('똑같은', 'Adjective'), ('함께', 'Adverb'), ('너무', 'Adverb'), ('붙잡히고', 'Verb'), ('예쁜', 'Adjective'), ('않니', 'Verb'), ('옛적', 'Noun'),\n",
      "Nearest to ('속', 'Noun'): ('가졌어요', 'Verb'), ('영리한', 'Adjective'), ('들어갔어요', 'Verb'), ('없겠다', 'Adjective'), ('위', 'Noun'), ('생각', 'Noun'), ('진기한', 'Adjective'), ('숨었고', 'Verb'),\n",
      "Nearest to ('새', 'Noun'): ('하얀', 'Adjective'), ('암탉', 'Noun'), ('모른', 'Verb'), ('푸드덕', 'Adverb'), ('아름다운', 'Adjective'), ('이', 'Noun'), ('묶음', 'Noun'), ('그', 'Noun'),\n",
      "Nearest to ('우리', 'Noun'): ('난', 'Noun'), ('함께', 'Adverb'), ('거니', 'Eomi'), ('저렇게', 'Adverb'), ('갈', 'Verb'), ('어서', 'Noun'), ('다', 'Adverb'), ('라며', 'Noun'),\n",
      "Nearest to ('지', 'Josa'): ('간청', 'Noun'), ('알아보곤', 'Verb'), ('윙윙', 'Noun'), ('왔고', 'Verb'), ('슬픈', 'Adjective'), ('올라갔지요', 'Verb'), ('아세요', 'Verb'), ('느끼곤', 'Verb'),\n",
      "Nearest to ('그', 'Noun'): ('에게', 'Josa'), ('그녀', 'Noun'), ('을', 'Josa'), ('사냥꾼', 'Noun'), ('이', 'Noun'), ('더', 'Noun'), ('맞아요', 'Verb'), ('하지만', 'Conjunction'),\n",
      "Nearest to ('길', 'Noun'): ('따라', 'Verb'), ('가는', 'Verb'), ('가며', 'Verb'), ('집의', 'Noun'), ('변한', 'Adjective'), ('그러다', 'Adjective'), ('나라', 'Noun'), ('그리하여', 'Conjunction'),\n",
      "Nearest to ('집', 'Noun'): ('둘', 'Noun'), ('빌', 'Verb'), ('함께', 'Adverb'), ('우리', 'Noun'), ('어둠', 'Noun'), ('여길', 'Verb'), ('오늘', 'Noun'), ('다가오더니', 'Verb'),\n",
      "Nearest to ('명', 'Noun'): ('중', 'Noun'), ('가난한', 'Adjective'), ('한', 'Verb'), ('마리', 'Noun'), ('옛날', 'Noun'), ('아내', 'Noun'), ('있었어요', 'Adjective'), ('포도주', 'Noun'),\n",
      "Nearest to ('하나', 'Noun'): ('한', 'Verb'), ('작은', 'Adjective'), ('개', 'Noun'), ('들을', 'Verb'), ('에게', 'Josa'), ('돌아와야', 'Verb'), ('자란', 'Verb'), ('애초', 'Noun'),\n",
      "Nearest to ('선', 'Noun'): ('달리는', 'Verb'), ('재봉사', 'Noun'), ('파고', 'Noun'), ('도망', 'Noun'), ('그럼에도', 'Adjective'), ('미친', 'Adjective'), ('사라지자', 'Verb'), ('흠뻑', 'Noun'),\n",
      "Nearest to ('방', 'Noun'): ('에게', 'Josa'), ('보더', 'Noun'), ('자르고', 'Verb'), ('또', 'Noun'), ('선반', 'Noun'), ('산이', 'Noun'), ('기어', 'Noun'), ('사라졌지요', 'Verb'),\n"
     ]
    }
   ],
   "source": [
    "num_iterations = input_li_size // batch_size\n",
    "print(\"number of iterations for each epoch :\", num_iterations)\n",
    "epochs = 10\n",
    "num_steps = num_iterations * epochs + 1\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized - Tensorflow\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(step, batch_size, input_li, output_li)\n",
    "\n",
    "        word_list = []\n",
    "        for word in batch_inputs:\n",
    "            word_list.append(word_to_pos_dict[word])\n",
    "\n",
    "        feed_dict = {}\n",
    "        for i in range(batch_size):\n",
    "            feed_dict[words_matrix[i]] = word_list[i]\n",
    "        feed_dict[train_inputs] = batch_inputs\n",
    "        feed_dict[train_labels] = batch_labels\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % (num_steps//10) == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % (num_steps//4) == 0:\n",
    "            pos_embed = pos_embeddings.eval()\n",
    "\n",
    "            # Print nearest words\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_pos = pos_reverse_dict[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % str(valid_pos)\n",
    "                for k in range(top_k):\n",
    "                    close_word = pos_reverse_dict[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, str(close_word))\n",
    "                print(log_str)\n",
    "\n",
    "    pos_embed = pos_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save vectors.\n",
    "def save_model(pos_list, embeddings, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(len(pos_list)))\n",
    "        f.write(\" \")\n",
    "        f.write(str(embedding_size))\n",
    "        f.write(\"\\n\")\n",
    "        for i in range(len(pos_list)):\n",
    "            pos = pos_list[i]\n",
    "            f.write(str(pos).replace(\"', '\", \"','\") + \" \")\n",
    "            f.write(' '.join(map(str, embeddings[i])))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Save vectors\n",
    "save_model(pos_li, pos_embed, \"pos.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
